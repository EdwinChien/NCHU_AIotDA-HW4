import os
import warnings
import streamlit as st

# 1. éš±è—æ‰€æœ‰ç‰ˆæœ¬ç›¸å®¹æ€§è­¦å‘Š (é‡å° urllib3, LibreSSL, Pydantic)
warnings.filterwarnings("ignore")

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# =========================
# Streamlit åŸºæœ¬è¨­å®š
# =========================
st.set_page_config(page_title="RAG ç©©å®šä¿®å¾©ç‰ˆ", layout="centered")
st.title("ğŸ“˜ ä¿®æ­£ç‰ˆ RAG æ–‡ä»¶å•ç­”ç³»çµ±")

#Token å®‰å…¨è®€å– (è«‹ç¢ºä¿å·²è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œæˆ–æ‰‹å‹•å¡«å…¥æ–° Token)
# å»ºè­°åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œ: export HUGGINGFACEHUB_API_TOKEN='æ‚¨çš„æ–°Token'
hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN")

if not hf_token:
    st.error("âŒ æ‰¾ä¸åˆ° HUGGINGFACEHUB_API_TOKEN ç’°å¢ƒè®Šæ•¸ï¼Œè«‹ç¢ºèªè¨­å®šã€‚")
    st.stop()

# =========================
# 1. è¼‰å…¥ LLM (è§£æ±º 410 Gone èˆ‡ StopIteration)
# =========================
@st.cache_resource
def load_llm():
    # ä½¿ç”¨ gemma-1.1 ç‰ˆæœ¬ï¼Œé€™åœ¨ç›®å‰å…è²» API ä¸Šé€£ç·šæœ€ç©©å®š
    repo_id = "google/gemma-1.1-2b-it"
    
    return HuggingFaceEndpoint(
        repo_id=repo_id,
        task="text-generation",
        max_new_tokens=512,
        temperature=0.1,
        huggingfacehub_api_token=hf_token,
        timeout=300 # é‡å° Mac SSL æ¡æ‰‹è¼ƒæ…¢å¢åŠ è¶…æ™‚æ™‚é–“
    )

llm = load_llm()

# =========================
# 2. å»ºç«‹ Vector DB (å‘é‡è³‡æ–™åº«)
# =========================
@st.cache_resource
def build_vector_db():
    pdf_path = "data/documents.pdf"
    if not os.path.exists(pdf_path):
        st.error(f"æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{pdf_path}ï¼Œè«‹ç¢ºèªç›®éŒ„çµæ§‹ã€‚")
        st.stop()
        
    # è¼‰å…¥ PDF
    loader = PyPDFLoader(pdf_path)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = loader.load_and_split(text_splitter)

    # åµŒå…¥æ¨¡å‹ (ä½¿ç”¨ HuggingFace è¨—ç®¡æ¨¡å‹)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    
    # å»ºç«‹è¨˜æ†¶é«”å‘é‡åº«
    return Chroma.from_documents(documents=docs, embedding=embeddings)

vectordb = build_vector_db()

# =========================
# 3. é…ç½® RAG æª¢ç´¢éˆ (é¿é–‹ Pydantic éŒ¯èª¤)
# =========================


# å®šç¾© Prompt ç¯„æœ¬
system_prompt = (
    "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©ç†ã€‚è«‹æ ¹æ“šæä¾›çš„ Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰å›ç­”å•é¡Œã€‚"
    "å¦‚æœç­”æ¡ˆä¸åœ¨ Context å…§ï¼Œè«‹å›ç­”ã€æˆ‘ä¸çŸ¥é“ã€ï¼Œä¸è¦ç·¨é€ ç­”æ¡ˆã€‚"
    "\n\nContext: {context}"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),
])

# å»ºç«‹æ–‡ä»¶çµ„åˆéˆèˆ‡æª¢ç´¢éˆ
combine_docs_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(vectordb.as_retriever(search_kwargs={"k": 3}), combine_docs_chain)

# =========================
# 4. UI äº’å‹•ä»‹é¢
# =========================
question = st.text_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šé€™ä»½æ–‡ä»¶çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ")

if question:
    with st.spinner("ğŸ” æ­£åœ¨æª¢ç´¢è³‡æ–™ä¸¦ç”Ÿæˆå›ç­”..."):
        try:
            # å‘¼å«æª¢ç´¢éˆ (æ–°ç‰ˆåƒæ•¸ç‚º input)
            response = rag_chain.invoke({"input": question})
            
            st.subheader("ğŸ¤– AI å›ç­”")
            st.success(response["answer"])

            with st.expander("ğŸ“„ æŸ¥çœ‹åƒè€ƒä¾†æºç‰‡æ®µ"):
                for i, doc in enumerate(response["context"]):
                    st.markdown(f"**ä¾†æº {i+1} (é ç¢¼: {doc.metadata.get('page', 'N/A')})**")
                    st.info(doc.page_content)
        except Exception as e:
            st.error(f"ç™¼ç”ŸåŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")
            st.info("æç¤ºï¼šå¦‚æœé‡åˆ° StopIterationï¼Œè«‹å˜—è©¦åŸ·è¡Œ 'pip install -U huggingface-hub'")