import streamlit as st
import os
import tempfile
import warnings
from langchain_community.document_loaders import PyPDFLoader
# âœ… æ–°ç‰ˆè·¯å¾‘ (å°æ‡‰ langchain-text-splitters å¥—ä»¶)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# å¿½ç•¥ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore")

# =========================
# é é¢é…ç½®
# =========================
st.set_page_config(page_title="RAG PDF åŠ©æ‰‹", layout="wide")
st.title("ğŸ¤– RAG æ™ºæ…§æ–‡ä»¶å°è©±ç³»çµ±")

# =========================
# Token ç®¡ç† (å„ªå…ˆè®€å– Secrets)
# =========================
# åœ¨ Streamlit Cloud çš„è¨­å®šé é¢ä¸­æ·»åŠ  Secrets: HUGGINGFACEHUB_API_TOKEN = "your_token"
hf_token = st.secrets.get("HUGGINGFACEHUB_API_TOKEN") or st.sidebar.text_input("è¼¸å…¥ HF Token", type="password")

if not hf_token:
    st.warning("âš ï¸ è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥ HuggingFace Token æˆ–åœ¨ Secrets ä¸­è¨­å®šã€‚")
    st.stop()

# =========================
# æ ¸å¿ƒåŠŸèƒ½çµ„ä»¶ (å¿«å–è™•ç†)
# =========================
@st.cache_resource
def load_llm(token):
    return HuggingFaceEndpoint(
        repo_id="google/gemma-1.1-2b-it",
        task="text-generation",
        max_new_tokens=512,
        temperature=0.1,
        huggingfacehub_api_token=token,
    )

@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    loader = PyPDFLoader(tmp_path)
    docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100))
    vectorstore = Chroma.from_documents(documents=docs, embedding=get_embeddings())
    os.remove(tmp_path) # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
    return vectorstore

# =========================
# å´é‚Šæ¬„èˆ‡æ–‡ä»¶è™•ç†
# =========================
with st.sidebar:
    st.header("ğŸ“„ æ–‡ä»¶ä¸Šå‚³")
    uploaded_file = st.file_uploader("ä¸Šå‚³ PDF é–‹å§‹å•ç­”", type="pdf")
    if st.button("æ¸…é™¤å°è©±ç´€éŒ„"):
        st.session_state.messages = []
        st.rerun()

if uploaded_file:
    if "vectordb" not in st.session_state:
        with st.spinner("æ­£åœ¨å»ºç«‹çŸ¥è­˜åº«..."):
            st.session_state.vectordb = process_pdf(uploaded_file)
            st.success("æ–‡ä»¶è™•ç†å®Œæˆï¼")

    # è¨­å®š RAG éˆ
    llm = load_llm(hf_token)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹åŠ©ç†ï¼Œè«‹æ ¹æ“š context å›ç­”å•é¡Œã€‚è‹¥ä¸çŸ¥é“è«‹èªªã€æˆ‘ä¸çŸ¥é“ã€ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\n\nContext: {context}"),
        ("human", "{input}"),
    ])
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(st.session_state.vectordb.as_retriever(), combine_docs_chain)

    # =========================
    # å°è©±ç•Œé¢ (Chat UI)
    # =========================
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # é¡¯ç¤ºæ­·å²è¨Šæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ä½¿ç”¨è€…è¼¸å…¥
    if prompt_input := st.chat_input("å° PDF å…§å®¹æå•..."):
        st.session_state.messages.append({"role": "user", "content": prompt_input})
        with st.chat_message("user"):
            st.markdown(prompt_input)

        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                response = rag_chain.invoke({"input": prompt_input})
                full_response = response["answer"]
                st.markdown(full_response)
                
                # é¡¯ç¤ºä¾†æº
                with st.expander("æŸ¥çœ‹åƒè€ƒä¾†æº"):
                    for doc in response["context"]:
                        st.caption(f"å†…å®¹ï¼š{doc.page_content[:200]}...")

        st.session_state.messages.append({"role": "assistant", "content": full_response})
else:
    st.info("è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶ä»¥å•Ÿç”¨å•ç­”åŠŸèƒ½ã€‚")
