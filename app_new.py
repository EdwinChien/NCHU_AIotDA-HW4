import streamlit as st
import os
import tempfile
import warnings
from huggingface_hub import InferenceClient

# æ ¸å¿ƒå¥—ä»¶
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

warnings.filterwarnings("ignore")

st.set_page_config(page_title="RAG ç©©å®šä¿®å¾©ç‰ˆ", layout="wide")
st.title("ğŸ“˜ RAG æ–‡ä»¶å•ç­”ç³»çµ± (API ä»»å‹™ä¿®æ­£ç‰ˆ)")

# å®‰å…¨è®€å– Token
hf_token = st.secrets.get("HUGGINGFACEHUB_API_TOKEN") or st.sidebar.text_input("HuggingFace Token", type="password")

if not hf_token:
    st.info("ğŸ‘‹ è«‹è¼¸å…¥æ‚¨çš„ HuggingFace API Token ä»¥é–‹å§‹ã€‚")
    st.stop()

# --- åˆå§‹åŒ–æ¨¡å‹èˆ‡å‘é‡åº« ---
@st.cache_resource
def get_embedding_model():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

@st.cache_resource
def get_inference_client(token):
    # ä½¿ç”¨ç›®å‰æœ€ç©©å®šçš„æ¨¡å‹
    return InferenceClient(model="HuggingFaceH4/zephyr-7b-beta", token=token)

# --- PDF è™•ç† ---
def process_pdf(file, _embeddings):
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(file.getvalue())
        loader = PyPDFLoader(tmp.name)
        docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100))
        vector_db = Chroma.from_documents(docs, _embeddings)
        os.remove(tmp.name)
    return vector_db

# --- ä»‹é¢é‚è¼¯ ---
embeddings = get_embedding_model()
client = get_inference_client(hf_token)

uploaded_file = st.file_uploader("ä¸Šå‚³ PDF æ–‡ä»¶", type="pdf")

if uploaded_file:
    if "db" not in st.session_state:
        with st.spinner("ğŸ“„ æ­£åœ¨å»ºç«‹ç´¢å¼•..."):
            st.session_state.db = process_pdf(uploaded_file, embeddings)
            st.success("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")

    retriever = st.session_state.db.as_retriever(search_kwargs={"k": 3})
    user_input = st.text_input("ğŸ’¬ è«‹é‡å°æ–‡ä»¶æå•ï¼š")

    if user_input:
        with st.spinner("ğŸ¤– æ€è€ƒä¸­..."):
            try:
                search_results = retriever.invoke(user_input)
                context_text = "\n\n".join([doc.page_content for doc in search_results])

                # ã€æ ¸å¿ƒä¿®æ­£ã€‘: æ”¹ç”¨ chat_completion ä¸¦å‚³å…¥ messages æ ¼å¼
                # é€™æ¨£èƒ½å®Œç¾æ”¯æ´ conversational ä»»å‹™èˆ‡å¤šç¨® Provider
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©æ‰‹ã€‚è«‹æ ¹æ“šå…§å®¹å›ç­”å•é¡Œï¼Œä¸¦ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚"},
                    {"role": "user", "content": f"å…§å®¹ï¼š{context_text}\n\nå•é¡Œï¼š{user_input}"}
                ]

                response = client.chat_completion(
                    messages=messages,
                    max_tokens=512,
                    temperature=0.2,
                )
                
                st.markdown("### ğŸ¤– AI å›ç­”")
                st.write(response.choices[0].message.content)
                
                with st.expander("ğŸ“„ æŸ¥çœ‹åƒè€ƒä¾†æº"):
                    for i, doc in enumerate(search_results):
                        st.caption(f"ä¾†æº {i+1}: {doc.page_content}")

            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
else:
    st.info("ğŸ‘ˆ è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶é–‹å§‹å°è©±ã€‚")
