import streamlit as st
import os
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

st.set_page_config(page_title="PDF RAG (LCEL Version)", layout="wide")
st.title("ğŸ“˜ ç©©å®šç‰ˆ RAG æ–‡ä»¶å•ç­” (LCEL)")

# å®‰å…¨è®€å– Token
hf_token = st.secrets.get("HUGGINGFACEHUB_API_TOKEN") or st.sidebar.text_input("HF Token", type="password")
if not hf_token:
    st.info("è«‹è¼¸å…¥ Token æˆ–åœ¨ Secrets è¨­å®šä¸­é…ç½®ã€‚")
    st.stop()

@st.cache_resource
def load_resources():
    # è¼‰å…¥ Embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    # è¼‰å…¥ LLM
    llm = HuggingFaceEndpoint(
        repo_id="google/gemma-1.1-2b-it",
        task="text-generation",
        max_new_tokens=512,
        huggingfacehub_api_token=hf_token
    )
    return embeddings, llm

embeddings_model, llm_model = load_resources()

# æ–‡ä»¶ä¸Šå‚³
uploaded_file = st.file_uploader("ä¸Šå‚³ PDF", type="pdf")

if uploaded_file:
    if "vector_db" not in st.session_state:
        with st.spinner("å»ºç«‹ç´¢å¼•ä¸­..."):
            with tempfile.NamedTemporaryFile(delete=False) as tmp:
                tmp.write(uploaded_file.getvalue())
                loader = PyPDFLoader(tmp.name)
                docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50))
                st.session_state.vector_db = Chroma.from_documents(docs, embeddings_model)
    
    # --- æ ¸å¿ƒé‚è¼¯ï¼šä½¿ç”¨ LCEL (é¿å…ä½¿ç”¨ langchain.chains) ---
    retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": 3})
    
    prompt = ChatPromptTemplate.from_template("""
    è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œï¼š
    {context}
    
    å•é¡Œï¼š{question}
    å›ç­”ï¼ˆè«‹ç”¨ç¹é«”ä¸­æ–‡ï¼‰ï¼š""")

    # å»ºç«‹ LCEL éˆ (é€™æ˜¯ä¸€å€‹åƒç®¡é“ä¸€æ¨£çš„æµï¼Œå®Œå…¨é¿é–‹èˆŠçš„ chains æ¨¡çµ„)
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm_model
        | StrOutputParser()
    )

    # UI äº’å‹•
    user_query = st.text_input("è¼¸å…¥æ‚¨çš„å•é¡Œï¼š")
    if user_query:
        with st.spinner("æ€è€ƒä¸­..."):
            result = rag_chain.invoke(user_query)
            st.markdown("### AI å›ç­”")
            st.write(result)
