import streamlit as st
import os
import tempfile
import warnings
from huggingface_hub import InferenceClient

# æ ¸å¿ƒåŸºç¤å¥—ä»¶ (ä½¿ç”¨ 2025 æœ€æ–°è·¯å¾‘)
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

warnings.filterwarnings("ignore")

st.set_page_config(page_title="RAG çµ‚æ¥µä¿®å¾©ç‰ˆ", layout="wide")
st.title("ğŸ“˜ RAG æ–‡ä»¶å•ç­”ç³»çµ± (API ç©©å®šç‰ˆ)")

# å®‰å…¨è®€å– Token
hf_token = st.secrets.get("HUGGINGFACEHUB_API_TOKEN") or st.sidebar.text_input("HuggingFace Token", type="password")

if not hf_token:
    st.info("ğŸ‘‹ è«‹è¼¸å…¥æ‚¨çš„ HuggingFace API Token ä»¥é–‹å§‹ã€‚")
    st.stop()

# --- åˆå§‹åŒ–æ¨¡å‹èˆ‡å‘é‡åº« ---
@st.cache_resource
def get_embedding_model():
    # ä½¿ç”¨ HuggingFace è¨—ç®¡çš„ Embedding æ¨¡å‹
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

@st.cache_resource
def get_inference_client(token):
    # ä½¿ç”¨ InferenceClient é¿é–‹æ‰€æœ‰ LangChain å°è£ Bug
    return InferenceClient(model="google/gemma-2-2b-it", token=token)

# --- PDF è™•ç† ---
def process_pdf(file, _embeddings):
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(file.getvalue())
        loader = PyPDFLoader(tmp.name)
        # å¢åŠ åˆ‡åˆ†å®¹éŒ¯
        docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100))
        # ä½¿ç”¨è¨˜æ†¶é«”å½¢å¼çš„ Chroma
        vector_db = Chroma.from_documents(docs, _embeddings)
        os.remove(tmp.name)
    return vector_db

# --- ä»‹é¢é‚è¼¯ ---
embeddings = get_embedding_model()
client = get_inference_client(hf_token)

uploaded_file = st.file_uploader("ä¸Šå‚³ PDF æ–‡ä»¶", type="pdf")

if uploaded_file:
    if "db" not in st.session_state:
        with st.spinner("ğŸ“„ æ­£åœ¨å»ºç«‹ç´¢å¼•..."):
            st.session_state.db = process_pdf(uploaded_file, embeddings)
            st.success("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")

    # å»ºç«‹æª¢ç´¢å™¨
    retriever = st.session_state.db.as_retriever(search_kwargs={"k": 3})

    user_input = st.text_input("ğŸ’¬ è«‹é‡å°æ–‡ä»¶æå•ï¼š")

    if user_input:
        with st.spinner("ğŸ¤– æ€è€ƒä¸­..."):
            try:
                # ã€é—œéµä¿®æ­£ã€‘: èˆŠç‰ˆ get_relevant_documents å·²æ£„ç”¨ï¼Œæ”¹ç”¨ invoke
                search_results = retriever.invoke(user_input)
                
                context_text = "\n\n".join([doc.page_content for doc in search_results])

                # æ§‹å»º Prompt
                prompt = f"è«‹æ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œï¼š\n\nå…§å®¹ï¼š{context_text}\n\nå•é¡Œï¼š{user_input}\n\nå›ç­”ï¼ˆè«‹ç”¨ç¹é«”ä¸­æ–‡ï¼‰ï¼š"

                # ä½¿ç”¨å®˜æ–¹ Client é€²è¡Œæ¨è«–
                response = client.text_generation(
                    prompt,
                    max_new_tokens=512,
                    temperature=0.1,
                    stop_sequences=["å•é¡Œï¼š"]
                )
                
                st.markdown("### ğŸ¤– AI å›ç­”")
                st.write(response)
                
                with st.expander("ğŸ“„ æŸ¥çœ‹åƒè€ƒä¾†æº"):
                    for i, doc in enumerate(search_results):
                        st.caption(f"ä¾†æº {i+1}: {doc.page_content}")

            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                st.info("æç¤ºï¼šå¦‚æœé‡åˆ°ç‰ˆæœ¬å•é¡Œï¼Œè«‹å˜—è©¦é»æ“Šå´é‚Šæ¬„çš„æ¸…é™¤å¿«å–ã€‚")
else:
    st.info("ğŸ‘ˆ è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶é–‹å§‹å°è©±ã€‚")
