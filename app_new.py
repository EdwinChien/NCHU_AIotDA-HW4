import streamlit as st
import os
import tempfile
import warnings
from huggingface_hub import InferenceClient

# æ ¸å¿ƒåŸºç¤å¥—ä»¶
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate

warnings.filterwarnings("ignore")

st.set_page_config(page_title="RAG çµ‚æ¥µä¿®å¾©ç‰ˆ", layout="wide")
st.title("ğŸ“˜ RAG æ–‡ä»¶å•ç­”ç³»çµ± (API ç©©å®šç‰ˆ)")

# å®‰å…¨è®€å– Token
hf_token = st.secrets.get("HUGGINGFACEHUB_API_TOKEN") or st.sidebar.text_input("HuggingFace Token", type="password")

if not hf_token:
    st.info("ğŸ‘‹ è«‹è¼¸å…¥æ‚¨çš„ HuggingFace API Token ä»¥é–‹å§‹ã€‚")
    st.stop()

# --- åˆå§‹åŒ–æ¨¡å‹èˆ‡å‘é‡åº« ---
@st.cache_resource
def get_embedding_model():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

@st.cache_resource
def get_inference_client(token):
    # æ›´æ›ç‚ºæ›´ç©©å®šçš„æ¨¡å‹ç¯€é»
    return InferenceClient(model="HuggingFaceH4/zephyr-7b-beta", token=token)

# --- PDF è™•ç† ---
def process_pdf(file, _embeddings):
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(file.getvalue())
        loader = PyPDFLoader(tmp.name)
        docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100))
        vector_db = Chroma.from_documents(docs, _embeddings)
        os.remove(tmp.name)
    return vector_db

# --- ä»‹é¢é‚è¼¯ ---
embeddings = get_embedding_model()
client = get_inference_client(hf_token)

uploaded_file = st.file_uploader("ä¸Šå‚³ PDF æ–‡ä»¶", type="pdf")

if uploaded_file:
    if "db" not in st.session_state:
        with st.spinner("ğŸ“„ æ­£åœ¨å»ºç«‹ç´¢å¼•..."):
            st.session_state.db = process_pdf(uploaded_file, embeddings)
            st.success("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")

    retriever = st.session_state.db.as_retriever(search_kwargs={"k": 3})
    user_input = st.text_input("ğŸ’¬ è«‹é‡å°æ–‡ä»¶æå•ï¼š")

    if user_input:
        with st.spinner("ğŸ¤– æ€è€ƒä¸­..."):
            try:
                search_results = retriever.invoke(user_input)
                context_text = "\n\n".join([doc.page_content for doc in search_results])

                # æ§‹å»º Prompt
                prompt = f"<|system|>\nè«‹æ ¹æ“šä»¥ä¸‹å…§å®¹å›ç­”å•é¡Œï¼Œä¸¦ä¸€å¾‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚</s>\n<|user|>\nå…§å®¹ï¼š{context_text}\nå•é¡Œï¼š{user_input}</s>\n<|assistant|>\n"

                # ä½¿ç”¨å®˜æ–¹ Client
                response = client.text_generation(
                    prompt,
                    max_new_tokens=512,
                    temperature=0.2,
                )
                
                st.markdown("### ğŸ¤– AI å›ç­”")
                st.write(response)
                
                with st.expander("ğŸ“„ æŸ¥çœ‹åƒè€ƒä¾†æº"):
                    for i, doc in enumerate(search_results):
                        st.caption(f"ä¾†æº {i+1}: {doc.page_content}")

            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
else:
    st.info("ğŸ‘ˆ è«‹å…ˆä¸Šå‚³ PDF æ–‡ä»¶é–‹å§‹å°è©±ã€‚")
