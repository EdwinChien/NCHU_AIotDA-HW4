import streamlit as st
import os
import tempfile
import warnings

# æ ¸å¿ƒåŸºç¤å¥—ä»¶
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

warnings.filterwarnings("ignore")

st.set_page_config(page_title="RAG ç©©å®šä¿®å¾©ç‰ˆ", layout="wide")
st.title("ğŸ“˜ RAG æ–‡ä»¶å•ç­”ç³»çµ± (API è·¯ç”±ä¿®æ­£ç‰ˆ)")

# å®‰å…¨è®€å– Token
hf_token = st.secrets.get("HUGGINGFACEHUB_API_TOKEN") or st.sidebar.text_input("HuggingFace Token", type="password")

if not hf_token:
    st.info("è«‹è¼¸å…¥ API Token ä»¥é–‹å§‹ã€‚")
    st.stop()

@st.cache_resource
def load_resources(token):
    # Embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    
    # è§£æ±º 410 Gone éŒ¯èª¤ï¼šä½¿ç”¨æœ€æ–°ç‰ˆçš„ HuggingFaceEndpoint
    # repo_id å»ºè­°ä½¿ç”¨ç©©å®šç‰ˆæœ¬ï¼Œä¾‹å¦‚ gemma-2-2b-it (é€™æ˜¯ç›®å‰çš„æ¨è–¦ç‰ˆ)
    repo_id = "google/gemma-2-2b-it" 
    
    llm = HuggingFaceEndpoint(
        repo_id=repo_id,
        task="text-generation",
        max_new_tokens=512,
        temperature=0.1,
        huggingfacehub_api_token=token,
        timeout=300
    )
    return embeddings, llm

embeddings_model, llm_model = load_resources(hf_token)

# --- æ–‡ä»¶è™•ç†èˆ‡å•ç­”é‚è¼¯ (ä¿æŒç©©å®šç‰ˆèªæ³•) ---
uploaded_file = st.file_uploader("é¸æ“‡ PDF æ–‡ä»¶", type="pdf")

if uploaded_file:
    if "db" not in st.session_state:
        with st.spinner("å»ºç«‹ç´¢å¼•ä¸­..."):
            with tempfile.NamedTemporaryFile(delete=False) as tmp:
                tmp.write(uploaded_file.getvalue())
                loader = PyPDFLoader(tmp.name)
                docs = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100))
                st.session_state.db = Chroma.from_documents(docs, embeddings_model)
                os.remove(tmp.name)

    retriever = st.session_state.db.as_retriever(search_kwargs={"k": 3})

    prompt = ChatPromptTemplate.from_template("""
    è«‹æ ¹æ“šä»¥ä¸‹ Context å›ç­”å•é¡Œã€‚è«‹ä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ã€å›ç­”ã€‚
    Context: {context}
    å•é¡Œ: {question}
    å›ç­”:""")

    rag_chain = (
        {"context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)), 
         "question": RunnablePassthrough()}
        | prompt
        | llm_model
        | StrOutputParser()
    )

    user_input = st.text_input("ğŸ’¬ è«‹è¼¸å…¥å•é¡Œï¼š")
    if user_input:
        with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
            try:
                # æŸäº›æ¨¡å‹ç”ŸæˆçµæœæœƒåŒ…å« promptï¼Œé€™è£¡åšç°¡å–®æ¸…æ´—
                response = rag_chain.invoke(user_input)
                st.markdown("### ğŸ¤– AI å›ç­”")
                st.write(response)
            except Exception as e:
                st.error(f"é€£ç·šå¤±æ•—ï¼š{str(e)}")
                st.info("æç¤ºï¼šå¦‚æœé‡åˆ° 410 éŒ¯èª¤ï¼Œè«‹ç¢ºèª requirements.txt ä¸­çš„ huggingface-hub ç‚ºæœ€æ–°ç‰ˆæœ¬ã€‚")
